!! Polyglot by example

In this chapter, I will guide you through the complete functionality of the Polyglot project by showing examples of how each of those features can be used. This is a documentation of Polyglot written in a form of storytelling with data.

!!! Brown Corpus
@brownCorpus

The brown corpus is a standard corpus used in NLP. The Brown Corpus was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.
It is a simple text file of raw text that we will load into Pharo to use. It can be downloaded from *Brown>https://github.com/olekscode/NgramModel/blob/master/Corpora/brown.txt*.

!!! Loading the data
To use this file of text we will need to load it into our Pharo image. Now let's say that I have downloaded the text file to our system. Now let's look at how to load it.

==Usage==
[[[
|file brown tokenizer tokens LM temp|

"Loading the Data into our Pharo image"
file := <path_to_training_corpus> asFileReference.
brown := file contents.
]]]


!!! Installation
To install Polyglot, go to the Playground (Ctrl\+O\+W) in your fresh Pharo image and execute the following Metacello script (select it and press Do-it button or Ctrl\+D):

[[[language=smalltalk
Metacello new
  baseline: 'Polyglot';
  repository: 'github://PolyMathOrg/Polyglot/src';
  load.
]]]

In all keyboard shortcuts mentioned in this booklet the ''Ctrl'' key is for Windows and Linux. On Mac OS, use ''Cmd'' instead.

!!!! Running the tests

First thing you should do after installing Polyglot is open the Polyglot-Tests package in Test Runner (Ctrl\+O\+U) or System Browser (Ctrl\+O\+B) and make sure that all tests are passing. Polyglot is tested with around 100 unit tests which provide 90\% code coverage. If you see some failing tests, please go to the Polyglot repository on GitHub and open a related issue.


!!! Tokenization

The first step in any natural language processing task is ''Tokenization''. 

Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences.
Tokenization in ''Polyglot'' can be done using the ''PGTokenizer'' package. ''PGTokenizer'' is the tokenization object class in ''Polyglot''.

==Usage== 
[[[
|file brown tokenizer tokens |

"Loading the Data into our Pharo image"
file := '/home/cyber/Desktop/brown.txt' asFileReference.
brown := file contents.

"Converting raw text into an array of tokens"
tokenizer := PGTokenizer new.
tokens := tokenizer tokenize: brown.
]]]

!!! Stemming
In linguistic morphology and information retrieval, stemming is the process of reducing inflected words to their word stem, base or root form. 
Stemming is used in tasks like building inverted indices for a search english. 
Let's take a small sample from our Brown corpus and apply stemming to all the words in that.

To do this ''PGPorterStemmer'' in the ''PGStemmer'' package in ''Polyglot''. This utilizes the base code from the Moose Information retrieval algos package for Pharo.

==Usage==
[[[
|file brown tokenizer tokens firstSent|

"Loading the Data into our Pharo image"
file := '/home/cyber/Desktop/brown.txt' asFileReference.
brown := file contents.

"Converting raw text into an array of tokens"
tokenizer := PGTokenizer new.
tokens := tokenizer tokenize: brown.

firstSent := tokens at: 1.
firstSent collect: [ :each | PGPorterStemmer stemOf: each ]
]]]

!!! Part of Speech Tagging (POS Tagging)
POS Tagging is the task of identifying the part-of-speech of every word in a particular sentence. The difficulty in the task lies in the fact that the same word in various sentences would be different parts of speech.
Part of Speech tagging can be done using ''PGPosTagger'' in ''Polyglot''.
Now let's say we want to POS Tag our Brown corpus.

==Usage==
[[[
|file brown tokenizer tokens firstSent tagger|

"Loading the Data into our Pharo image"
file := '/home/cyber/Desktop/brown.txt' asFileReference.
brown := file contents.

"Converting raw text into an array of tokens"
tokenizer := PGTokenizer new.
tokens := tokenizer sentenceTokenize: brown.

firstSent := tokens at: 1.
tagger := PGNltkPosTagger new.
tagger raw_parse: firstSent
]]]

!!! Named Entity Recognition (NER)
Named-entity recognition is the task of locating and classifying named entity mentions in unstructured text into categories like names, organizations, locations, time expressions, etc. 
